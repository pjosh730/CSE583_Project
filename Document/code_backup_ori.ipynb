{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "63de4277-c079-1630-2e47-44d138f3e7bf"
   },
   "source": [
    "# Library Import #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "193a91a4-7404-a01b-2c78-6b7385979b9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor,LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import six\n",
    "from abc import ABCMeta\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import defaultdict\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras import backend as K\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f98318d0-c122-24a1-a9a6-62ab93985428"
   },
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c39b53ac-2080-bfb4-124d-5e775af2cec2"
   },
   "source": [
    "we'll use all of the dates up to the end of 2014 as our training data and everything after as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "28217d5a-e580-fa92-fdc2-45a4842812e0"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../input/Combined_News_DJIA.csv')\n",
    "train = data[data['Date'] < '2015-01-01']\n",
    "test = data[data['Date'] > '2014-12-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d372c26-3c4c-7890-3089-c8ddb17ca81a"
   },
   "source": [
    "# Data Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b43212e0-9344-a216-b86b-37cfbfb37752"
   },
   "source": [
    "First, we transform the string of news into the  number of words as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "a87f0d3d-80f3-af90-9368-dedde8c1154f"
   },
   "outputs": [],
   "source": [
    "trainheadlines = []\n",
    "for row in range(0,len(train.index)):\n",
    "    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "66913298-12f3-da89-030a-90ba38b1e11b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 31675)\n"
     ]
    }
   ],
   "source": [
    "basicvectorizer = CountVectorizer()\n",
    "basictrain = basicvectorizer.fit_transform(trainheadlines)\n",
    "print(basictrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4647ab33-fd7e-0d98-9afd-7ea21eb02cf8"
   },
   "source": [
    "## Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ee566614-2f26-df99-3c54-700a07cf83bd"
   },
   "source": [
    "### Logic Regression 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc0ac1c7-f4fe-3bbe-18fc-f578947626b1"
   },
   "source": [
    "Algorithm: Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5a6e98cc-ec7f-3dd2-5dce-2c2c04cff260"
   },
   "source": [
    "Input: the counts of single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "5adfb2da-7467-505b-d44b-0a028e60227b"
   },
   "outputs": [],
   "source": [
    "basicmodel = LogisticRegression()\n",
    "basicmodel = basicmodel.fit(basictrain, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "56d1ea73-641b-6fdf-698f-32518fe99dec"
   },
   "outputs": [],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "basictest = basicvectorizer.transform(testheadlines)\n",
    "preds1 = basicmodel.predict(basictest)\n",
    "acc1=accuracy_score(test['Label'], preds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "bc13f327-70bd-2f8f-4ab7-6a9dfae356e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Regression 1 accuracy:  0.425925925926\n"
     ]
    }
   ],
   "source": [
    "print('Logic Regression 1 accuracy: ',acc1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c08a6f88-d723-97b5-dd09-a61b1f3f7829"
   },
   "source": [
    "The accuracy is only 0.42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "2303559d-4ad8-7d30-ff3d-ee3bb91440ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19419</th>\n",
       "      <td>0.497924</td>\n",
       "      <td>nigeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25261</th>\n",
       "      <td>0.452526</td>\n",
       "      <td>self</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29286</th>\n",
       "      <td>0.428011</td>\n",
       "      <td>tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>0.425863</td>\n",
       "      <td>korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20135</th>\n",
       "      <td>0.425716</td>\n",
       "      <td>olympics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Coefficient      Word\n",
       "19419     0.497924   nigeria\n",
       "25261     0.452526      self\n",
       "29286     0.428011        tv\n",
       "15998     0.425863     korea\n",
       "20135     0.425716  olympics"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basicwords = basicvectorizer.get_feature_names()\n",
    "basiccoeffs = basicmodel.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : basicwords, \n",
    "                        'Coefficient' : basiccoeffs})\n",
    "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "coeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "949e33a5-f63d-0059-2ee0-17e625aede36"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16949</th>\n",
       "      <td>-0.463116</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>-0.470454</td>\n",
       "      <td>begin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25433</th>\n",
       "      <td>-0.494555</td>\n",
       "      <td>sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24754</th>\n",
       "      <td>-0.549725</td>\n",
       "      <td>sanctions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24542</th>\n",
       "      <td>-0.587794</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Coefficient       Word\n",
       "16949    -0.463116        low\n",
       "3651     -0.470454      begin\n",
       "25433    -0.494555        sex\n",
       "24754    -0.549725  sanctions\n",
       "24542    -0.587794        run"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6aa2253b-0cf6-4853-4ff5-6143dae71fb1"
   },
   "source": [
    "### Logic Regression 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "65239831-43f2-290b-8e21-b4a9f82e5a54"
   },
   "source": [
    "Algorithm: Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a108baea-371d-d93f-7d65-213ecd4337ec"
   },
   "source": [
    "Input: the counts of phrases with two connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "15df5336-f6f9-cd66-b020-6072aff3083b"
   },
   "source": [
    "We delete phrases of which frequency lower than 0.03 or higher than 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "dbdf7b4f-4673-49f1-d510-377026c95611"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.97, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b6bf747c-021c-eb7f-cd50-2240138ed555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 657)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "8c225d22-d997-5db8-877e-03022a0785b4"
   },
   "outputs": [],
   "source": [
    "advancedmodel = LogisticRegression()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "1b3421b9-b326-4ef5-4235-2e764e5cd3a3"
   },
   "outputs": [],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds2 = advancedmodel.predict(advancedtest)\n",
    "acc2=accuracy_score(test['Label'], preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "422fb025-f985-e010-3e9e-2ee8c4494c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Regression 2 accuracy:  0.571428571429\n"
     ]
    }
   ],
   "source": [
    "print('Logic Regression 2 accuracy: ', acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8f5a4ca4-75a0-6a24-2c34-61855d2b102a"
   },
   "source": [
    "The accuracy is higher than input of single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "3af2a0fb-a8ef-84f2-2a80-3568d327baec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1.264377</td>\n",
       "      <td>set to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.259669</td>\n",
       "      <td>and other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1.216202</td>\n",
       "      <td>right to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1.166895</td>\n",
       "      <td>likely to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.128663</td>\n",
       "      <td>after the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Coefficient      Words\n",
       "411     1.264377     set to\n",
       "31      1.259669  and other\n",
       "391     1.216202   right to\n",
       "276     1.166895  likely to\n",
       "14      1.128663  after the"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1fd8c2b-23fd-7dda-cf8d-0381c3d0736f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>-1.098193</td>\n",
       "      <td>fire on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-1.128193</td>\n",
       "      <td>around the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>-1.144952</td>\n",
       "      <td>phone hacking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>-1.146688</td>\n",
       "      <td>up in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>-1.393351</td>\n",
       "      <td>the country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Coefficient          Words\n",
       "131    -1.098193        fire on\n",
       "40     -1.128193     around the\n",
       "366    -1.144952  phone hacking\n",
       "597    -1.146688          up in\n",
       "452    -1.393351    the country"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39b0d7a4-cedc-d51c-11f4-d3a1d454682f"
   },
   "source": [
    "### Logic Regression 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4cf79ce1-930d-474a-f07b-dd397f9a72eb"
   },
   "source": [
    "Algorithm: Logic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a62cefba-3f14-ce21-49a5-1fdd09556dde"
   },
   "source": [
    "Input: the counts of phrases with three connected words(exclude words which are too common like \"a\" ,\"an\" ,\"the\" and words too uncommon of which counts are too small )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "886f33fe-d47f-d545-1b73-29f1e3618e01"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.0039, max_df=0.1, max_features = 200000, ngram_range = (3, 3))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "5437284a-b14e-96cd-e6d5-97c41790d42a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 2282)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "5e29bfca-0041-5b86-fdff-9c502f4aa1ad"
   },
   "outputs": [],
   "source": [
    "advancedmodel = LogisticRegression()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "d5717cfb-3f8c-1af4-665b-6a22e6e9c9ba"
   },
   "outputs": [],
   "source": [
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds3 = advancedmodel.predict(advancedtest)\n",
    "acc3 = accuracy_score(test['Label'], preds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "b700896c-936b-004d-2d70-50fefee2129c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic Regression 3 accuracy:  0.555555555556\n"
     ]
    }
   ],
   "source": [
    "print('Logic Regression 3 accuracy: ', acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2a438254-8acb-4db7-4f38-bb07be962dc2"
   },
   "source": [
    "The accuracy is lower than input of phrases with two connected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "66fee024-03b4-5964-f925-cfb3746a4741"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>1.076893</td>\n",
       "      <td>to the us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0.886762</td>\n",
       "      <td>in china the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>0.853562</td>\n",
       "      <td>the right to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>0.845685</td>\n",
       "      <td>nobel peace prize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>0.832537</td>\n",
       "      <td>first time since</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Coefficient              Words\n",
       "2075     1.076893          to the us\n",
       "686      0.886762       in china the\n",
       "1775     0.853562       the right to\n",
       "1070     0.845685  nobel peace prize\n",
       "448      0.832537   first time since"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "c50f97d8-a6f7-0a60-9edf-42bb8b7aa492"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>-0.912953</td>\n",
       "      <td>said to be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>-0.933243</td>\n",
       "      <td>in west africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>-0.943155</td>\n",
       "      <td>phone hacking scandal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>-0.972078</td>\n",
       "      <td>around the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>-1.154834</td>\n",
       "      <td>in the country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Coefficient                  Words\n",
       "1425    -0.912953             said to be\n",
       "838     -0.933243         in west africa\n",
       "1317    -0.943155  phone hacking scandal\n",
       "152     -0.972078       around the world\n",
       "779     -1.154834         in the country"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a4229efa-49d5-82fb-5a1d-e1ca4001e0ac"
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ccc2e71-28c6-f425-f229-f1395293bffc"
   },
   "source": [
    "### NBayes 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "476ba54c-4b47-6029-e0de-29e6fcb64845"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.7, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "1ce37cf1-5c09-9fe7-09ec-a23d6d3b749e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 529)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "4726d110-1821-ea71-dc5c-7a21ad628dd1"
   },
   "outputs": [],
   "source": [
    "advancedmodel = MultinomialNB(alpha=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds4 = advancedmodel.predict(advancedtest)\n",
    "acc4=accuracy_score(test['Label'], preds4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "ff1f469d-1c12-5a3d-1aed-148179c26824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBayes 1 accuracy:  0.513227513228\n"
     ]
    }
   ],
   "source": [
    "print('NBayes 1 accuracy: ', acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "02a9958e-ab18-bf74-58f1-160c5b47cbff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>-5.180422</td>\n",
       "      <td>israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-5.252936</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-5.312797</td>\n",
       "      <td>china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>-5.317049</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>-5.329156</td>\n",
       "      <td>says</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Coefficient   Words\n",
       "234    -5.180422  israel\n",
       "497    -5.252936     was\n",
       "94     -5.312797   china\n",
       "318    -5.317049     not\n",
       "394    -5.329156    says"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "6223b466-48eb-0a0a-5d36-c082e50b1fe5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>-6.948273</td>\n",
       "      <td>held</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>-6.951188</td>\n",
       "      <td>tells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>-6.952040</td>\n",
       "      <td>story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>-6.970522</td>\n",
       "      <td>monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>-6.970642</td>\n",
       "      <td>lost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Coefficient   Words\n",
       "204    -6.948273    held\n",
       "446    -6.951188   tells\n",
       "426    -6.952040   story\n",
       "290    -6.970522  monday\n",
       "268    -6.970642    lost"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dcc814d2-014d-5cb9-b106-52ff648d9c18"
   },
   "source": [
    "### NBayes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "dfe60202-6348-d724-c860-c3b46c66c447"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "28b21c32-28de-1bc4-161e-dcf63737caca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 631)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "23ab2ed6-8e9a-4291-2f9b-6fc2563f33ca"
   },
   "outputs": [],
   "source": [
    "advancedmodel = MultinomialNB(alpha=0.0001)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds5 = advancedmodel.predict(advancedtest)\n",
    "acc5 = accuracy_score(test['Label'], preds5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "aa7d48c3-60cc-0895-7ed3-ddbf796339ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBayes 2 accuracy:  0.542328042328\n"
     ]
    }
   ],
   "source": [
    "print('NBayes 2 accuracy: ', acc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "f49cb3c4-b80b-48a8-69be-ac6c579e2ed4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-5.495760</td>\n",
       "      <td>at least</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-5.499764</td>\n",
       "      <td>in china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>-5.543152</td>\n",
       "      <td>it is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>-5.556370</td>\n",
       "      <td>killed in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>-5.594222</td>\n",
       "      <td>the united</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Coefficient       Words\n",
       "49     -5.495760    at least\n",
       "185    -5.499764    in china\n",
       "247    -5.543152       it is\n",
       "259    -5.556370   killed in\n",
       "487    -5.594222  the united"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advwords = advancedvectorizer.get_feature_names()\n",
    "advcoeffs = advancedmodel.coef_.tolist()[0]\n",
    "advcoeffdf = pd.DataFrame({'Words' : advwords, \n",
    "                        'Coefficient' : advcoeffs})\n",
    "advcoeffdf = advcoeffdf.sort_values(['Coefficient', 'Words'], ascending=[0, 1])\n",
    "advcoeffdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "087f9b18-c50b-17b5-60b6-2cf2dada7c3c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>-7.176794</td>\n",
       "      <td>say the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>-7.190919</td>\n",
       "      <td>been arrested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>-7.212155</td>\n",
       "      <td>if it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-7.218639</td>\n",
       "      <td>children in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>-7.323029</td>\n",
       "      <td>said the</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Coefficient          Words\n",
       "386    -7.176794        say the\n",
       "65     -7.190919  been arrested\n",
       "172    -7.212155          if it\n",
       "84     -7.218639    children in\n",
       "383    -7.323029       said the"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advcoeffdf.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4bfcc58e-1ed4-18d5-a321-e58a4744dd8c"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a77b2afd-5654-aaff-74a9-0013fe4e8164"
   },
   "source": [
    "### RF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "91fe0b03-37a0-eb19-2632-cdc04067b12f"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.01, max_df=0.99, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "878756f6-f57d-88cb-fe82-5181c1e40901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 4565)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "b561af90-db7d-ef58-26a2-5721d5432615"
   },
   "outputs": [],
   "source": [
    "advancedmodel = RandomForestClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds6 = advancedmodel.predict(advancedtest)\n",
    "acc6 = accuracy_score(test['Label'], preds6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "91fe3c5f-b005-97f0-da7c-597c29521877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 1 accuracy:  0.52380952381\n"
     ]
    }
   ],
   "source": [
    "print('RF 1 accuracy: ', acc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "76a971f3-878d-33e2-249a-a2b063aa5460"
   },
   "source": [
    "### RF 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "1db5461b-1ee9-2196-6725-66a6c2376a19"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "e1fb8019-c981-9d18-8f54-2750387ec071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 631)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "bcddf034-4a10-2026-55ea-dd3d969a297b"
   },
   "outputs": [],
   "source": [
    "advancedmodel = RandomForestClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds7 = advancedmodel.predict(advancedtest)\n",
    "acc7 = accuracy_score(test['Label'], preds7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "e395e3a6-7bc8-a434-16af-be13aebbabbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF 2 accuracy:  0.539682539683\n"
     ]
    }
   ],
   "source": [
    "print('RF 2 accuracy: ', acc7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0a12329a-12c7-aa90-a138-025087dd76ba"
   },
   "source": [
    "## Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea1681ab-166f-20a6-3c29-f34f84438a36"
   },
   "source": [
    "### GBM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "4ec11f0e-6c50-8149-93c3-acd17d1e7caa"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.9, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "3c5d312d-555f-f353-606e-2f560631033a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 541)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "8f9d72ff-fb8e-5d1a-6016-3ebdb76a2ffd"
   },
   "outputs": [],
   "source": [
    "advancedmodel = GradientBoostingClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds8 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc8 = accuracy_score(test['Label'], preds8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "ba8ae675-7e71-e24e-69bf-99d5d19590d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 1 accuracy:  0.473544973545\n"
     ]
    }
   ],
   "source": [
    "print('GBM 1 accuracy: ', acc8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e39653db-00b9-d1e0-d82e-87105e0eba1e"
   },
   "source": [
    "### GBM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "d35d6a94-69f4-5370-6d0f-5b98c91d2565"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.02, max_df=0.175, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "b83bc122-5059-665a-28c1-85145787155e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 1169)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "e98ca989-15e6-b785-f282-4e59f849cd1f"
   },
   "outputs": [],
   "source": [
    "advancedmodel = GradientBoostingClassifier()\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds9 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc9 = accuracy_score(test['Label'], preds9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "_cell_guid": "6df5f34f-a315-c1c4-fdbe-c54127468d5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM 2 accuracy:  0.547619047619\n"
     ]
    }
   ],
   "source": [
    "print('GBM 2 accuracy: ', acc9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b7bd5500-e077-8225-31b5-a9c3f1015ca5"
   },
   "source": [
    "## Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8f139d81-25c5-ba11-8924-e68b6e2e55f6"
   },
   "source": [
    "### SGDClassifier 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "_cell_guid": "d071bb2f-df6a-1657-1af8-5d31ee8e4a19"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.2, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_cell_guid": "24d0156a-2705-fece-001e-0426da57d383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 202)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "30e4b846-af7e-1eda-dade-f1550d6e043b"
   },
   "outputs": [],
   "source": [
    "advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds10 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc10 = accuracy_score(test['Label'], preds10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "79a91d72-1cd5-4308-9b0e-9810314cdd1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier 1:  0.505291005291\n"
     ]
    }
   ],
   "source": [
    "print('SGDClassifier 1: ', acc10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f46cdfe-20b2-9bc6-103a-1f44e2172789"
   },
   "source": [
    "### SGDClassifier 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_cell_guid": "8ccd7ace-7326-4dfe-b93d-a8377e1a3ba6"
   },
   "outputs": [],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.03, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "c012b95d-8575-ef34-d699-d240ade631f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 631)\n"
     ]
    }
   ],
   "source": [
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "5e4f2994-e199-976d-f54b-0e676b6b11be"
   },
   "outputs": [],
   "source": [
    "advancedmodel = SGDClassifier(loss='modified_huber', n_iter=5, random_state=0, shuffle=True)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds11 = advancedmodel.predict(advancedtest.toarray())\n",
    "acc11 = accuracy_score(test['Label'], preds11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "a9d8499e-ae6a-6197-c86f-c31bf73f3f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier 2:  0.542328042328\n"
     ]
    }
   ],
   "source": [
    "print('SGDClassifier 2: ', acc11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6aa0178-e7f3-6427-f88f-aa6e436fc87d"
   },
   "source": [
    "## Naive Bayes SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "_cell_guid": "cd1391ca-1b70-13c0-3a62-151fc84f6069"
   },
   "outputs": [],
   "source": [
    "class NBSVM(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "\n",
    "    def __init__(self, alpha=1.0, C=1.0, max_iter=10000):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.svm_ = [] # fuggly\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, 'csr')\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # so we don't have to cast X to floating point\n",
    "        Y = Y.astype(np.float64)\n",
    "\n",
    "        # Count raw events from data\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.ratios_ = np.full((n_effective_classes, n_features), self.alpha,\n",
    "                                 dtype=np.float64)\n",
    "        self._compute_ratios(X, Y)\n",
    "\n",
    "        # flugglyness\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            svm = LinearSVC(C=self.C, max_iter=self.max_iter)\n",
    "            Y_i = Y[:,i]\n",
    "            svm.fit(X_i, Y_i)\n",
    "            self.svm_.append(svm) \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_effective_classes = self.class_count_.shape[0]\n",
    "        n_examples = X.shape[0]\n",
    "\n",
    "        D = np.zeros((n_effective_classes, n_examples))\n",
    "\n",
    "        for i in range(n_effective_classes):\n",
    "            X_i = X.multiply(self.ratios_[i])\n",
    "            D[i] = self.svm_[i].decision_function(X_i)\n",
    "        \n",
    "        return self.classes_[np.argmax(D, axis=0)]\n",
    "        \n",
    "    def _compute_ratios(self, X, Y):\n",
    "        \"\"\"Count feature occurrences and compute ratios.\"\"\"\n",
    "        if np.any((X.data if issparse(X) else X) < 0):\n",
    "            raise ValueError(\"Input X must be non-negative\")\n",
    "\n",
    "        self.ratios_ += safe_sparse_dot(Y.T, X)  # ratio + feature_occurrance_c\n",
    "        normalize(self.ratios_, norm='l1', axis=1, copy=False)\n",
    "        row_calc = lambda r: np.log(np.divide(r, (1 - r)))\n",
    "        self.ratios_ = np.apply_along_axis(row_calc, axis=1, arr=self.ratios_)\n",
    "        check_array(self.ratios_)\n",
    "        self.ratios_ = sparse.csr_matrix(self.ratios_)\n",
    "\n",
    "        #p_c /= np.linalg.norm(p_c, ord=1)\n",
    "        #ratios[c] = np.log(p_c / (1 - p_c))\n",
    "\n",
    "\n",
    "def f1_class(pred, truth, class_val):\n",
    "    n = len(truth)\n",
    "\n",
    "    truth_class = 0\n",
    "    pred_class = 0\n",
    "    tp = 0\n",
    "\n",
    "    for ii in range(0, n):\n",
    "        if truth[ii] == class_val:\n",
    "            truth_class += 1\n",
    "            if truth[ii] == pred[ii]:\n",
    "                tp += 1\n",
    "                pred_class += 1\n",
    "                continue;\n",
    "        if pred[ii] == class_val:\n",
    "            pred_class += 1\n",
    "\n",
    "    precision = tp / float(pred_class)\n",
    "    recall = tp / float(truth_class)\n",
    "\n",
    "    return (2.0 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def semeval_senti_f1(pred, truth, pos=2, neg=0): \n",
    "\n",
    "    f1_pos = f1_class(pred, truth, pos)\n",
    "    f1_neg = f1_class(pred, truth, neg)\n",
    "\n",
    "    return (f1_pos + f1_neg) / 2.0;\n",
    "\n",
    "\n",
    "def main(train_file, test_file, ngram=(1, 3)):\n",
    "    print('loading...')\n",
    "    train = pd.read_csv(train_file, delimiter='\\t', encoding='utf-8', header=0,\n",
    "                        names=['text', 'label'])\n",
    "\n",
    "    # to shuffle:\n",
    "    #train.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "    test = pd.read_csv(test_file, delimiter='\\t', encoding='utf-8', header=0,\n",
    "                        names=['text', 'label'])\n",
    "\n",
    "    print('vectorizing...')\n",
    "    vect = CountVectorizer()\n",
    "    classifier = NBSVM()\n",
    "\n",
    "    # create pipeline\n",
    "    clf = Pipeline([('vect', vect), ('nbsvm', classifier)])\n",
    "    params = {\n",
    "        'vect__token_pattern': r\"\\S+\",\n",
    "        'vect__ngram_range': ngram, \n",
    "        'vect__binary': True\n",
    "    }\n",
    "    clf.set_params(**params)\n",
    "\n",
    "    #X_train = vect.fit_transform(train['text'])\n",
    "    #X_test = vect.transform(test['text'])\n",
    "\n",
    "    print('fitting...')\n",
    "    clf.fit(train['text'], train['label'])\n",
    "\n",
    "    print('classifying...')\n",
    "    pred = clf.predict(test['text'])\n",
    "   \n",
    "    print('testing...')\n",
    "    acc = accuracy_score(test['label'], pred)\n",
    "    f1 = semeval_senti_f1(pred, test['label'])\n",
    "    print('NBSVM: acc=%f, f1=%f' % (acc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f0e000a-ca1a-5611-0ca8-c8bc40eafecb"
   },
   "source": [
    "### NBSVM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "e054e5ea-c8be-cb98-90a1-2512b3478b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 535)\n"
     ]
    }
   ],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.1, max_df=0.8, max_features = 200000, ngram_range = (1, 1))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "_cell_guid": "b57a2b30-d69f-cf0a-05d0-8a065d5a758b"
   },
   "outputs": [],
   "source": [
    "advancedmodel = NBSVM(C=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds12 = advancedmodel.predict(advancedtest)\n",
    "acc12 = accuracy_score(test['Label'], preds12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_cell_guid": "6acdade4-665e-5c06-95af-3e5ddcfd33d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBSVM 1:  0.478835978836\n"
     ]
    }
   ],
   "source": [
    "print('NBSVM 1: ', acc12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aca410f2-118d-f973-6b14-a63048444f26"
   },
   "source": [
    "### NBSVM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_cell_guid": "663e5e98-7ccd-29a4-40a8-10a46327c06b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 613)\n"
     ]
    }
   ],
   "source": [
    "advancedvectorizer = TfidfVectorizer( min_df=0.031, max_df=0.2, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "_cell_guid": "0c67fed4-8110-2a52-bb33-1da6f294b592"
   },
   "outputs": [],
   "source": [
    "advancedmodel = NBSVM(C=0.01)\n",
    "advancedmodel = advancedmodel.fit(advancedtrain, train[\"Label\"])\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "preds13 = advancedmodel.predict(advancedtest)\n",
    "acc13 = accuracy_score(test['Label'], preds13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_cell_guid": "803861e3-d740-c2ed-2b2e-57221ba9b8c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBSVM 2:  0.595238095238\n"
     ]
    }
   ],
   "source": [
    "print('NBSVM 2: ', acc13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "682e897f-169c-8694-d05d-66e9fc844158"
   },
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1de5a061-52d5-2578-9cb0-2602499ad87b"
   },
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_cell_guid": "ac32628f-8080-5d53-e3a2-3d6986431f15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 401)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_classes = 2\n",
    "advancedvectorizer = TfidfVectorizer( min_df=0.04, max_df=0.3, max_features = 200000, ngram_range = (2, 2))\n",
    "advancedtrain = advancedvectorizer.fit_transform(trainheadlines)\n",
    "testheadlines = []\n",
    "for row in range(0,len(test.index)):\n",
    "    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n",
    "advancedtest = advancedvectorizer.transform(testheadlines)\n",
    "print(advancedtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_cell_guid": "eca75f7e-4fe5-0be4-85ee-47fecf3ad4d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1611, 401)\n",
      "X_test shape: (378, 401)\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/Keras-1.2.2-py3.6.egg/keras/models.py:654: UserWarning: The \"show_accuracy\" argument is deprecated, instead you should pass the \"accuracy\" metric to the model at compile time:\n",
      "`model.compile(optimizer, loss, metrics=[\"accuracy\"])`\n",
      "  warnings.warn('The \"show_accuracy\" argument is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1369 samples, validate on 242 samples\n",
      "Epoch 1/2\n",
      "1369/1369 [==============================] - 0s - loss: 0.6953 - val_loss: 0.6879\n",
      "Epoch 2/2\n",
      "1369/1369 [==============================] - 0s - loss: 0.6685 - val_loss: 0.6850\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "X_train = advancedtrain.toarray()\n",
    "X_test = advancedtest.toarray()\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(train[\"Label\"])\n",
    "y_test = np.array(test[\"Label\"])\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "# pre-processing: divide by max and substract mean\n",
    "scale = np.max(X_train)\n",
    "X_train /= scale\n",
    "X_test /= scale\n",
    "\n",
    "mean = np.mean(X_train)\n",
    "X_train -= mean\n",
    "X_test -= mean\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Here's a Deep Dumb MLP (DDMLP)\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=input_dim))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# we'll use categorical xent for the loss, and RMSprop as the optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "print(\"Training...\")\n",
    "model.fit(X_train, Y_train, nb_epoch=2, batch_size=16, validation_split=0.15, show_accuracy=True)\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds14 = model.predict_classes(X_test, verbose=0)\n",
    "acc14 = accuracy_score(test[\"Label\"], preds14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_cell_guid": "39345ebd-2cf2-8332-6fb2-d00bc801af57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.547619047619\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "143c5d6d-4259-c945-f1dd-b07a4c388526"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_cell_guid": "db232b70-f3a0-0952-e7ab-9c0b768e8025"
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.1\n",
    "maxlen = 200\n",
    "batch_size = 32\n",
    "nb_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "e5f6e70a-325d-5a94-0c16-c852f951d475"
   },
   "outputs": [],
   "source": [
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(nb_words=max_features)\n",
    "tokenizer.fit_on_texts(trainheadlines)\n",
    "sequences_train = tokenizer.texts_to_sequences(trainheadlines)\n",
    "sequences_test = tokenizer.texts_to_sequences(testheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_cell_guid": "a9224d6a-3bca-ef84-9817-a72b41241f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (1611, 200)\n",
      "X_test shape: (378, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_cell_guid": "29359f0d-8d42-2038-c04f-2cd5bc87bc93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1611 samples, validate on 378 samples\n",
      "Epoch 1/3\n",
      "1611/1611 [==============================] - 50s - loss: 0.6942 - acc: 0.5208 - val_loss: 0.6935 - val_acc: 0.5079\n",
      "Epoch 2/3\n",
      "1611/1611 [==============================] - 51s - loss: 0.6722 - acc: 0.5971 - val_loss: 0.6904 - val_acc: 0.5106\n",
      "Epoch 3/3\n",
      "1611/1611 [==============================] - 50s - loss: 0.5941 - acc: 0.7492 - val_loss: 0.6831 - val_acc: 0.5661\n",
      "378/378 [==============================] - 3s     \n",
      "Test score: 0.683092702633\n",
      "Test accuracy: 0.566137566138\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=3,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds15 = model.predict_classes(X_test, verbose=0)\n",
    "acc15 = accuracy_score(test['Label'], preds15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_cell_guid": "365d4d4b-57cd-c678-6e62-133aa32586fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.566137566138\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8afb0d07-7d0d-dd18-b19c-be603f301b52"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_cell_guid": "7271a1d1-cde0-a26c-de99-edfb7dfbf918"
   },
   "outputs": [],
   "source": [
    "nb_filter = 120\n",
    "filter_length = 2\n",
    "hidden_dims = 120\n",
    "nb_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_cell_guid": "c071e2fc-2106-4bc5-2e2f-dde4a84dabdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "model.add(Lambda(max_1d, output_shape=(nb_filter,)))\n",
    "model.add(Dense(hidden_dims)) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_cell_guid": "61e07990-43ef-ff27-3b36-dca146f8b4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1611 samples, validate on 378 samples\n",
      "Epoch 1/1\n",
      "1611/1611 [==============================] - 14s - loss: 0.6900 - acc: 0.5317 - val_loss: 0.6974 - val_acc: 0.5079\n",
      "352/378 [==========================>...] - ETA: 0sTest score: 0.69742725829\n",
      "Test accuracy: 0.507936509829\n",
      "Generating test predictions...\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "\n",
    "print(\"Generating test predictions...\")\n",
    "preds16 = model.predict_classes(X_test, verbose=0)\n",
    "acc16 = accuracy_score(test['Label'], preds16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_cell_guid": "49e9abbe-d224-b757-c17b-bd4cfa5da818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy:  0.507936507937\n"
     ]
    }
   ],
   "source": [
    "print('prediction accuracy: ', acc16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "21b86295-1a3b-a1b8-78cf-b032b9d74eaa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "91aa05fb-b8fe-fe99-d191-c08b8dc5620c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "60fa6ce4-1658-11d2-c0ad-3b43a542fff0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "03273578-b025-4e87-6088-a993fce31e2f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 1261,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
